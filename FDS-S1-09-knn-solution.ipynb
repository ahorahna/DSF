{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inf2 - Foundations of Data Science\n",
    "## S1 Week 09: Scikit-learn - _k_-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning outcomes:** \n",
    "In this lab you will learn how to apply _k_-Nearest Neighbours (_k_-NN) to a data set using the scikit-learn library. By the end of the lab you should be able to:\n",
    "\n",
    "- explain how _k_ can be chosen appropriately, \n",
    "- explain the importance between training, testing and validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end of semester is near, you have worked hard, and some might think about celebrating it with a nice dinner and a glass of wine (other beverages are available). In this lab, we will try to predict the quality of wine based on several characteristics, and test whether price alone is a good predictor of good wine.\n",
    "\n",
    "**Data set information:** The data set is taken from [UCI](https://archive.ics.uci.edu/ml/datasets/wine+quality), but was originally used in [Cortez et al. 2009](https://www.sciencedirect.com/science/article/pii/S0167923609001377)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#Importing sklearn functions\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. About _k_-nearest neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_k_-NN is a supervised machine learning algorithm. _k_-NN can be used for regression as well as classification problems. In this lab, we will use it for classification, as each wine has one of a number of distinct ratings that we want to predict.\n",
    "\n",
    "Suppose we are presented with a previously unseen data point, but we do not know its class. _k_-nearest neighbours predicts that the class (or label) of this point depends on the labels of the _k_ data points closest to it.  This definition prompts two questions:\n",
    "\n",
    "- How do we chose the number of neighbours?\n",
    "- How do we weight the \"votes\" of each neighbour on the classification. It seems reasonable that a neighbour closer to the unseen data point should have a larger weight than one which is further away.\n",
    "\n",
    "There are numerous ways to measure how \"close\" two data points are. A common choice is the Euclidean distance, which is the square root of the sum of squares of the different coordinates of two data points, or in a formula: \n",
    "\n",
    "$\\mathrm{euclideanDistance}((x_1, y_1),(x_2, y_2))= \\sqrt{ (x_1-x_2)^2+(y_1-y_2)^2 }$\n",
    "\n",
    "However, there are many more options to chose from:\n",
    "- Manhattan distance (sum of absolute values of differences between points)\n",
    "- Minkowski distance (the generalization of the Euclidean distance and Manhattan distance, by taking the $p$-th root and $p$-th power of the differences)\n",
    "- Cosine distance (looking at the angle formed by two points and the origin).\n",
    "\n",
    "To get a better idea of what the cosine distance is, and how it differs to the Euclidean distance, we can recommend this [blog post](https://cmry.github.io/notes/euclidean-v-cosine).\n",
    "\n",
    "**Exercise 01:**\n",
    "\n",
    "Before we get into how to chose the value of $k$, let us think about an example where, depending on $k$, the assignment of the cluster to a new data point will change. You can assume for this exercise that we do not weight the importance of the data point differently depending on the distance to the queried point.\n",
    "\n",
    "- Sketch on paper or plot a scatter plot that contains the following parts\n",
    "  - a number of data points (training data) each of which is labeled with one of two classes (e.g. red and blue, or noughts and crosses)\n",
    "  - a test data point (i.e. a data point with no label) that would be assigned to the red class for one value of $k$ (e.g. $k=1$) and the blue class for another value of $k$ (e.g. $k=3$)\n",
    "  - two circles around the queried point, each of which encapsulates all the neighbours taken into account for two different values of $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1_x = [0.5, 0.5, 1.3, 2.4, 2, 2, 2.5]\n",
    "cluster_1_y = [2, 3, 1.5, 2, 2.5, 3.2, 2.5]\n",
    "cluser_2_x = [2.5, 3, 2.7, 4, 4.5, 4.5, 5]\n",
    "cluster_2_y = [1.4, 0.5, 1.8, 1.5, 0.7, 2.5, 2]\n",
    "query = [2.5, 1.7]\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "ax.scatter(cluster_1_x, cluster_1_y)\n",
    "ax.scatter(cluser_2_x, cluster_2_y)\n",
    "ax.plot(query[0], query[1], marker='x')\n",
    "circle1 = plt.Circle((2.5, 1.7), 0.5, color='k', fill=False)\n",
    "circle2 = plt.Circle((2.5, 1.7), 1, color='k', fill=False)\n",
    "ax.add_patch(circle1)\n",
    "ax.add_patch(circle2)\n",
    "# This will assign the query point to cluster 2 with K=3, and to cluster 1 with K=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "Discuss with your lab partner what happens when you choose $k$ too small and what happens when you choose $k$ too large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "- Choosing $k$ too small leads to overfitting and your model will be susceptible to noise.\n",
    "- Choosing $k$ too large leads to underfitting and your model will have a very simple boundary, and in the worst case consider all new data points as from the same class, which has most data entries in your data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to answer these points in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Data exploration and visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 02:**\n",
    "\n",
    "- Load and examine the `'winequality-red.csv'` data set.\n",
    "\n",
    "It has a column `'quality'` with values between 3 and 8. Our goal is to only differentiate between low quality, medium quality, and high quality.\n",
    "\n",
    "- Print out how many data entries you have per quality score.\n",
    "\n",
    "It seems like there are very few very poor and excellent wines.\n",
    "\n",
    "- Edit the scores in your dataset, such that all wines with quality 3-5 get score 0, wines with scores 6 get score 1, and all wines with better scores get a score 2. \n",
    "- Print out how many wines are in each new category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv(os.path.join(os.getcwd(), 'datasets', 'winequality-red.csv'))\n",
    "wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in range(3,9):\n",
    "    print('There are ' + str(len(wine[wine['quality'] == score])) + ' wines with score ' + str(score))\n",
    "\n",
    "# A one-line version of the abov is:\n",
    "print(wine['quality'].value_counts())\n",
    "    \n",
    "wine.loc[wine.quality < 6, 'quality'] = 0\n",
    "wine.loc[(wine.quality == 6), 'quality'] = 1\n",
    "wine.loc[(wine.quality > 6), 'quality'] = 2\n",
    "\n",
    "\n",
    "print('\\nBased on the new scoring scheme:\\n')\n",
    "for score in range(0,3):\n",
    "    print('There are ' + str(len(wine[wine['quality'] == score])) + ' wines with score ' + str(score))\n",
    "    \n",
    "# Or, a one-line version of the above:\n",
    "print(wine['quality'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 03:**\n",
    "\n",
    "- Visualise the edited data using a seaborn pairplot, using hue to indicate quality. (Note: this may take up to a minute to run.)\n",
    "- Discuss how clear the distinction between the classes is visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "sns.pairplot(data=wine, hue='quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The distributions of some features (e.g. alcohol, sulphates and volatile acidity) appear to differ somewhat for the the different qualities.\n",
    "- There doesn't appear to be any pair of features for which the three classes are neatly separated, but some pairs (e.g. free sulfur dioxide and alcohol) suggest that it might be possible to find decision boundaries that classify the data with some level of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Data preparation\n",
    "\n",
    "In order to use scikit-learn's supervised learning algorithms, we need to separate data into features and labels. \n",
    "\n",
    "**Exercise 04:**\n",
    "\n",
    "- Store the `quality` score column (the label) in a new series, which you could call `wine_label`.\n",
    "- Store the features (i.e. every column apart from `quality`) in a new data frame, which you could call `wine_features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "wine_labels = wine['quality']\n",
    "wine_features = wine.drop(columns=['quality'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Standardisation\n",
    "\n",
    "It often makes sense to standardise features for distance-based algorithms, such as _k_-NN. The definition of standardised variables was coverd in the Topic on Linear Models, and the PCA lab. Here we won't write our own function, but instead use the scikit learn function.\n",
    "\n",
    "**Exercise 05:**\n",
    "\n",
    "- Look at the documentation for scikit-learn's [`StandardScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?highlight=standardscaler#sklearn.preprocessing.StandardScaler) class\n",
    "- Use the `StandardScaler()` to create standardised features you have just created, and save these standardised features in a new variable, e.g. `wine_features_standardised`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "standardiser = StandardScaler()\n",
    "wine_features_standardised = standardiser.fit_transform(wine_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Training and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distinction between the classes in the pairplot does not seem very clear. We will be running _k_-NN to classify the wines, but before that we need to create training and testing sets, which is your job. \n",
    "\n",
    "**Exercise 06:**\n",
    "- Split the standardised features you created in Exercise 05 into two sets, such that the training data contains 80% of the data and the test data contains 20%. You can use [sklearn's](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) native function.#\n",
    "- We suggest that you call the variables produced by the train-test split: \n",
    "  - wine_features_standardised_train\n",
    "  - wine_labels_train\n",
    "  - wine_features_standardised_test\n",
    "  - wine_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_features_standardised_train, wine_features_standardised_test, wine_labels_train, wine_labels_test = train_test_split(wine_features_standardised,\n",
    "                     wine_labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. Running k-nearest neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn has a class that allows us to build _k_-NN classifiers, called [`KNeighborsClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html?highlight=kneighbors#sklearn.neighbors.KNeighborsClassifier). As with every other algorithm scikit-learn implements, this class has  `fit()` and `predict()` methods. After we've instantiated a model object, we train it by  passing the training data to the `fit()` method.  We can then  determine the classification of an unseen data point by passing the point to   the `predict()` method. \n",
    "\n",
    "Remember from the Topic *Intro to supervised learning*, that a classifier is a function that takes a feature vector $\\mathbf{x}$ and returns a class $c$. The `predict()` method applied to a trained scikit-learn classifier model therefore acts as a classifier.\n",
    "\n",
    "**Exercise 07:**\n",
    "\n",
    "- Create a _k_-NN classifier in scikit-learn by following these steps:\n",
    "    1. Create an instance of [`KNeighborsClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html?highlight=kneighbors#sklearn.neighbors.KNeighborsClassifier)\n",
    "    2. `.fit()` it with your training data (`wine_features_standardised_train` and `wine_labels_train`)\n",
    "\n",
    "- Then `.predict()` the class of each point in the test set from its features. \n",
    "- Compare the predictions with label in the test set.\n",
    "- Try computing the error rate and accuracy of the classifier (see the topics on classification and evaluation). You can write the code yourself or look at [`metrics.accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html?highlight=accuracy_score#sklearn.metrics.accuracy_score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(wine_features_standardised_train, wine_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'predicted': knn.predict(wine_features_standardised_test),\n",
    "              'actual': wine_labels_test})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy by coding\n",
    "np.sum(df['predicted'] == df['actual'])/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy by scikit-learn\n",
    "metrics.accuracy_score(wine_labels_test, knn.predict(wine_features_standardised_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G. Choosing _k_\n",
    "\n",
    "Remember from the topic *Intro to supervised learning*, that a hyperparameter is a number that controls how an algorithm learns and predicts. In _k_-nearest neighbours, $k$ is a hyperparameter. You have already discussed what the problems might be by choosing a bad $k$. So how do we choose $k$? Sadly the answer is that there is no simple answer, and it will depend on your data. The way of getting to the optimal solution is heuristic.\n",
    "\n",
    "For example, you can run the _k_-NN algorithm with different values of $k$, and plot the result on a graph, where on one axis, you have the values of $k$ and on the other axis you have the accuracy of the model. \n",
    "\n",
    "We set hyperparameters in scikit-learn by passing parameters to the model objects. For example, we set the numbers in an `KNeighborsClassifier()`, by passing the argument `n_neighbors=k` when we create the object; `n_neighbors` is set to 5 by default. \n",
    "\n",
    "We can also set the metric which we use to compute which neighbours are closest to a new data point. By default it is `metric='minkowski'`, with `p`, the parameter of the Minkowski distance, set to 2. Thus, by default sklearn uses the Euclidean distance as a metric to compute the closest neighbours.\n",
    "\n",
    "Finally, you can choose whether or not the distance of each neighbour should be used as a weight, to decide which neighbour's class should be weighted more. You can set it to `weights='distance'`, otherwise each neighbour will be weighted the same.\n",
    "\n",
    "**Exercise 08:**\n",
    "\n",
    "- Run _k_-NN 100 times each time with a different $k$. Predict the classes of your test set, and compute the accuracy of each model. \n",
    "- Plot $k$ vs the accuracy of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "accuracy = []\n",
    "for k in range(1, 100):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(wine_features_standardised_train, wine_labels_train)\n",
    "    wine_predict = knn.predict(wine_features_standardised_test)\n",
    "    accuracy.append(metrics.accuracy_score(wine_labels_test, wine_predict))\n",
    "\n",
    "plt.plot(range(1, 100), accuracy)\n",
    "# plt.ylim([0, 1]) # We can discuss where the baseline should be!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "You can now see for which $k$ you have the highest accuracy. The goal of machine learning, however, is that it generalizes well. Is the best model you found above, also the best model in general for new incoming data? Will you and your lab partner have the same plot? If you have several models with similar accuracy, which model would you choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer:\n",
    "\n",
    "- The plot above only shows the accuracy for one specific test data set, and might strongly vary on different test sets.\n",
    "- If several models have a similar good accuracy, one should lean towards the simpler model, as a more complex model might overfit to data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H. Cross validation\n",
    "\n",
    "If one of your models achieved an accuracy of 80%, does that mean that you would get an accuracy of 80% on any unseen data? The answer is no. You will need to test it on unseen data first. However, the problem is that we have already used up all the data. \n",
    "\n",
    "This leads to the concept of validation, and the important difference between validation and test data. So the first step will be to split our data into three data sets: training data, validation data and test data (which is equivalent to future unseen data).\n",
    "\n",
    "The problem is that if we split up our data, for example, 60% training data, 20% validation data, and 20% test data (which we are not allowed to touch until the end), we loose 20% of the data, which we have previously used for training. That's where _K_-fold cross-validation comes into play. \n",
    "\n",
    "We first split the data set into training and testing data, and set the test set aside (e.g. 80-20 ratio). Then, we split the remaining training data randomly into $K$ equal parts. We've covered cross-validation briefly in the lectures, but there's also a [helpful description in the scikit-learn documentation](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation).\n",
    "\n",
    "**Remark:** Sadly the letters $k$ and $K$ are overloaded in the machine learning literature. In this lab, $k$ refers to the neighbours and capital $K$ refers to the number of splits we perform on the training data, but in the literature $k$ is often used for both.\n",
    "\n",
    "We train our model $K$ times, such that we set one of the $K$ subsets aside (which is the validation data), train with the remaining $K-1$ parts of the training data, and report the accuracy, by predicting on the set aside validation data set. Then, we report the mean of all $K$ loops. We do this for all $K$ models. Then, we choose the model that had the best accuracy, and finally report the accuracy of our model, by predicting the outcome of the test data set, which we have set aside at the very beginning. \n",
    "\n",
    "If we choose $K=4$ we get the ratio back, we initial split it up with (i.e. 60-20-20).\n",
    "\n",
    "Scikit-learn has a function that does all that for us: `cross_val_score()`, which we have already loaded above. The first parameter is the model object (you do not need to fit it in advance - the function does it automatically) you want to cross validate (you can use cross-validation for any learning algorithm). The second parameter is the training data (i.e. the 80% you have set aside in the beginning). The third parameter is the associated target value. Then, we specify with `cv=K` the number of folds we want to use, and finally, we specify based on what we want to score our models, e.g. `scoring='accuracy'`. The return value is an array of scores for each fold.\n",
    "\n",
    "**Exercise 09:**\n",
    "\n",
    "- Run cross-validation on your training data with 4 folds.\n",
    "- Compute the mean of the accuracy of each fold.\n",
    "- Plot the mean accuracy of each model.\n",
    "- Choose the best model and report the accuracy of that model on the test data. (Careful: This time you need to fit it first.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "k_range = range(1,100)\n",
    "k_scores=[]\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k) \n",
    "    accuracy = cross_val_score(knn, wine_features_standardised_train, wine_labels_train, cv=4, scoring='accuracy') \n",
    "    k_scores.append(accuracy.mean())\n",
    "    \n",
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-validated accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "We used the accuracy to score a model. What else could we score it on?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "You have now trained your model several times. Discuss with your lab partner what the limitations and drawbacks of _k_-NN are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. PCA and classification\n",
    "\n",
    "**Exercise 10 (Optional):**\n",
    "\n",
    "It can be helpful to use PCA to visualise data. \n",
    "\n",
    "- Apply PCA to the standardised features.\n",
    "- Plot the scatter plot of first two PCs.\n",
    "- You could also try making a seaborn pairplot of 3 or more PCs.\n",
    "- Do the classes look more separable?\n",
    "\n",
    "We can also use PCA as a pre-processing step.\n",
    "- Would k-NN work better on features derived from PCA? If so, what is the best number of PCs to retain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "pca = PCA(n_components=11).fit(wine_features_standardised)\n",
    "pca_result = pca.transform(wine_features_standardised)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(x=pca_result[:, 0], y=pca_result[:, 1], c=wine_labels)\n",
    "ax.legend(*scatter.legend_elements(),loc=\"upper right\", title=\"Quality\")\n",
    "ax.set_xlabel('First principal component')\n",
    "ax.set_ylabel('Second principal component')\n",
    "ax.set_title('PCA of Wine Quality')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We don't have answers for rest of this question - it's up to you to investigate"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
